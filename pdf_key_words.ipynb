{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toxresearch/lit_search/blob/main/pdf_key_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b8e2b52",
      "metadata": {
        "id": "1b8e2b52",
        "outputId": "8ed673fd-9fc7-4a16-e2d4-4824740eea37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: PyPDF2 in c:\\users\\miao.li\\appdata\\roaming\\python\\python39\\site-packages (3.0.1)\n",
            "Requirement already satisfied: textract in c:\\users\\miao.li\\appdata\\roaming\\python\\python39\\site-packages (1.6.5)\n",
            "Requirement already satisfied: PdfReader in c:\\users\\miao.li\\appdata\\roaming\\python\\python39\\site-packages (0.1.15)\n",
            "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.7)\n",
            "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from PyPDF2) (4.3.0)\n",
            "Requirement already satisfied: six~=1.12.0 in c:\\users\\miao.li\\appdata\\roaming\\python\\python39\\site-packages (from textract) (1.12.0)\n",
            "Requirement already satisfied: pdfminer.six==20191110 in c:\\users\\miao.li\\appdata\\roaming\\python\\python39\\site-packages (from textract) (20191110)\n",
            "Requirement already satisfied: beautifulsoup4~=4.8.0 in c:\\users\\miao.li\\appdata\\roaming\\python\\python39\\site-packages (from textract) (4.8.2)\n",
            "Requirement already satisfied: SpeechRecognition~=3.8.1 in c:\\users\\miao.li\\appdata\\roaming\\python\\python39\\site-packages (from textract) (3.8.1)\n",
            "Requirement already satisfied: xlrd~=1.2.0 in c:\\users\\miao.li\\appdata\\roaming\\python\\python39\\site-packages (from textract) (1.2.0)\n",
            "Requirement already satisfied: extract-msg<=0.29.* in c:\\users\\miao.li\\appdata\\roaming\\python\\python39\\site-packages (from textract) (0.28.7)\n",
            "Requirement already satisfied: argcomplete~=1.10.0 in c:\\users\\miao.li\\appdata\\roaming\\python\\python39\\site-packages (from textract) (1.10.3)\n",
            "Requirement already satisfied: chardet==3.* in c:\\users\\miao.li\\appdata\\roaming\\python\\python39\\site-packages (from textract) (3.0.4)\n",
            "Requirement already satisfied: python-pptx~=0.6.18 in c:\\users\\miao.li\\appdata\\roaming\\python\\python39\\site-packages (from textract) (0.6.23)\n",
            "Requirement already satisfied: docx2txt~=0.8 in c:\\users\\miao.li\\appdata\\roaming\\python\\python39\\site-packages (from textract) (0.8)\n",
            "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
            "Requirement already satisfied: pycryptodome in c:\\users\\miao.li\\appdata\\roaming\\python\\python39\\site-packages (from pdfminer.six==20191110->textract) (3.20.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from PdfReader) (2.8.2)\n",
            "Requirement already satisfied: bitarray>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from PdfReader) (2.5.1)\n",
            "Requirement already satisfied: pillow>=7.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from PdfReader) (9.2.0)\n",
            "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
            "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
            "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: soupsieve>=1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4~=4.8.0->textract) (2.3.1)\n",
            "Requirement already satisfied: imapclient==2.1.0 in c:\\users\\miao.li\\appdata\\roaming\\python\\python39\\site-packages (from extract-msg<=0.29.*->textract) (2.1.0)\n",
            "Requirement already satisfied: ebcdic>=1.1.1 in c:\\users\\miao.li\\appdata\\roaming\\python\\python39\\site-packages (from extract-msg<=0.29.*->textract) (1.1.1)\n",
            "Requirement already satisfied: tzlocal>=2.1 in c:\\users\\miao.li\\appdata\\roaming\\python\\python39\\site-packages (from extract-msg<=0.29.*->textract) (5.2)\n",
            "Requirement already satisfied: olefile>=0.46 in c:\\programdata\\anaconda3\\lib\\site-packages (from extract-msg<=0.29.*->textract) (0.46)\n",
            "Requirement already satisfied: compressed-rtf>=1.0.6 in c:\\users\\miao.li\\appdata\\roaming\\python\\python39\\site-packages (from extract-msg<=0.29.*->textract) (1.0.6)\n",
            "Requirement already satisfied: lxml>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-pptx~=0.6.18->textract) (4.9.1)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-pptx~=0.6.18->textract) (3.0.3)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n",
            "Requirement already satisfied: tzdata in c:\\users\\miao.li\\appdata\\roaming\\python\\python39\\site-packages (from tzlocal>=2.1->extract-msg<=0.29.*->textract) (2024.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install PyPDF2 textract PdfReader nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c00439d",
      "metadata": {
        "id": "5c00439d",
        "outputId": "d8c17a7e-6a6c-4072-b672-eaeb886d07e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     keywords  number_of_times_word_appeared        tf       idf    tf_idf\n",
            "750       the                            288  0.046340 -5.662960 -0.262419\n",
            "1502       of                            223  0.035881 -5.407172 -0.194014\n",
            "1628      and                            159  0.025583 -5.068904 -0.129679\n",
            "697        in                            128  0.020595 -4.852030 -0.099929\n",
            "712         a                            127  0.020434 -4.844187 -0.098988\n",
            "...       ...                            ...       ...       ...       ...\n",
            "819      each                             11  0.001770 -2.397895 -0.004244\n",
            "1564   tested                             11  0.001770 -2.397895 -0.004244\n",
            "416   tissues                             11  0.001770 -2.397895 -0.004244\n",
            "1385  acetone                             11  0.001770 -2.397895 -0.004244\n",
            "477    indoor                             11  0.001770 -2.397895 -0.004244\n",
            "\n",
            "[100 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "#GPT-3 revision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import PyPDF2\n",
        "import textract\n",
        "import re\n",
        "\n",
        "filename = 'Abdallah et al. 2015.pdf'\n",
        "\n",
        "# Open the PDF file\n",
        "with open(filename, 'rb') as pdfFileObj:\n",
        "    pdfReader = PyPDF2.PdfReader(pdfFileObj)\n",
        "    num_pages = len(pdfReader.pages)\n",
        "\n",
        "    text = \"\"\n",
        "\n",
        "    # Extract text from each page\n",
        "    for count in range(num_pages):\n",
        "        pageObj = pdfReader.pages[count]\n",
        "        page_text = pageObj.extract_text()  # Use get_text() in the latest PyPDF2 versions\n",
        "        if page_text:\n",
        "            text += page_text\n",
        "\n",
        "# If PyPDF2 fails to extract text, use textract as a fallback\n",
        "if not text.strip():\n",
        "    text = textract.process(filename, method='tesseract', language='eng').decode('utf-8')\n",
        "\n",
        "# Clean and prepare the text\n",
        "text = text.lower()  # Convert to lowercase\n",
        "keywords = re.findall(r'\\b\\w+\\b', text)  # Extract words using regex\n",
        "\n",
        "# Create a DataFrame with unique keywords\n",
        "df = pd.DataFrame(list(set(keywords)), columns=['keywords'])\n",
        "\n",
        "# Define the weightage function\n",
        "def weightage(word, text, number_of_documents=1):\n",
        "    word_list = re.findall(r'\\b' + re.escape(word) + r'\\b', text)\n",
        "    number_of_times_word_appeared = len(word_list)\n",
        "    tf = number_of_times_word_appeared / float(len(text.split()))\n",
        "    idf = np.log((number_of_documents) / float(number_of_times_word_appeared))\n",
        "    tf_idf = tf * idf\n",
        "    return number_of_times_word_appeared, tf, idf, tf_idf\n",
        "\n",
        "# Calculate tf-idf and other metrics for each keyword\n",
        "df['number_of_times_word_appeared'] = df['keywords'].apply(lambda x: weightage(x, text)[0])\n",
        "df['tf'] = df['keywords'].apply(lambda x: weightage(x, text)[1])\n",
        "df['idf'] = df['keywords'].apply(lambda x: weightage(x, text)[2])\n",
        "df['tf_idf'] = df['keywords'].apply(lambda x: weightage(x, text)[3])\n",
        "\n",
        "# Sort the DataFrame by tf-idf\n",
        "df = df.sort_values('tf_idf', ascending=True)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('out_put.csv', index=False)\n",
        "print(df.head(100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f21303cb",
      "metadata": {
        "id": "f21303cb",
        "outputId": "50f51945-638b-4d76-c4ea-79a8af367241"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Abdallah et al. 2015.pdf and saved to Abdallah et al. 2015_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Abrego et al. 2016.pdf and saved to Abrego et al. 2016_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Alsaab et al. 2015.pdf and saved to Alsaab et al. 2015_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Berthet et al. 2020.pdf and saved to Berthet et al. 2020_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Boonen et al. 2012.pdf and saved to Boonen et al. 2012_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Bányiová et al. 2016.pdf and saved to Bányiová et al. 2016_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Champmartin et al. 2020.pdf and saved to Champmartin et al. 2020_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\De Spiegeleer et al. 2013.pdf and saved to De Spiegeleer et al. 2013_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Ellison et al. 2020 Corrigendum.pdf and saved to Ellison et al. 2020 Corrigendum_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Ellison et al. 2020.pdf and saved to Ellison et al. 2020_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Elmoslemany et al. 2012.pdf and saved to Elmoslemany et al. 2012_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Fox et al. 2014.pdf and saved to Fox et al. 2014_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Franko et al 2012.pdf and saved to Franko et al 2012_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Frasch et al. 2016.pdf and saved to Frasch et al. 2016_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Frasch et al. 2017.pdf and saved to Frasch et al. 2017_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Haltner‐Ukomadu et al. 2012.pdf and saved to Haltner‐Ukomadu et al. 2012_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Haq et al. 2018.pdf and saved to Haq et al. 2018_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Hopf et al. 2019.pdf and saved to Hopf et al. 2019_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Hui et al. 2011.pdf and saved to Hui et al. 2011_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Kopečná et al. 2017.pdf and saved to Kopečná et al. 2017_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Kopečná et al. 2019.pdf and saved to Kopečná et al. 2019_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Krishnan et al. 2012.pdf and saved to Krishnan et al. 2012_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Mohammed et al. 2013.pdf and saved to Mohammed et al. 2013_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Mohyeldin et al. 2016.pdf and saved to Mohyeldin et al. 2016_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Moore et al. 2014.pdf and saved to Moore et al. 2014_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Morris et al. 2019.pdf and saved to Morris et al. 2019_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Musazzi et al. 2015.pdf and saved to Musazzi et al. 2015_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Namjoshi et al. 2014.pdf and saved to Namjoshi et al. 2014_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Nielsen et al. 2012.pdf and saved to Nielsen et al. 2012_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Santos et al. 2019.pdf and saved to Santos et al. 2019_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Ternullo et al. 2017.pdf and saved to Ternullo et al. 2017_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Uchida et al. 2015.pdf and saved to Uchida et al. 2015_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Veryser et al. 2014 2.pdf and saved to Veryser et al. 2014 2_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Veryser et al. 2014.pdf and saved to Veryser et al. 2014_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Wen et al. 2011.pdf and saved to Wen et al. 2011_output.csv\n",
            "Processed C:/Users/Miao.Li/python_learning/pdf scraping\\Xenikakis et al. 2019.pdf and saved to Xenikakis et al. 2019_output.csv\n"
          ]
        }
      ],
      "source": [
        "# run batch\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import PyPDF2\n",
        "import textract\n",
        "import re\n",
        "\n",
        "# Directory containing the PDF files\n",
        "folder_path = 'C:/Users/Miao.Li/python_learning/pdf scraping'\n",
        "\n",
        "# Function to process a single PDF file\n",
        "def process_pdf(filename):\n",
        "    with open(filename, 'rb') as pdfFileObj:\n",
        "        pdfReader = PyPDF2.PdfReader(pdfFileObj)\n",
        "        num_pages = len(pdfReader.pages)\n",
        "\n",
        "        text = \"\"\n",
        "\n",
        "        # Extract text from each page\n",
        "        for count in range(num_pages):\n",
        "            pageObj = pdfReader.pages[count]\n",
        "            page_text = pageObj.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text\n",
        "\n",
        "    # If PyPDF2 fails to extract text, use textract as a fallback\n",
        "    if not text.strip():\n",
        "        text = textract.process(filename, method='tesseract', language='eng').decode('utf-8')\n",
        "\n",
        "    # Clean and prepare the text\n",
        "    text = text.lower()\n",
        "    keywords = re.findall(r'\\b\\w+\\b', text)\n",
        "\n",
        "    # Create a DataFrame with unique keywords\n",
        "    df = pd.DataFrame(list(set(keywords)), columns=['keywords'])\n",
        "\n",
        "    # Define the weightage function\n",
        "    def weightage(word, text, number_of_documents=1):\n",
        "        word_list = re.findall(r'\\b' + re.escape(word) + r'\\b', text)\n",
        "        number_of_times_word_appeared = len(word_list)\n",
        "        tf = number_of_times_word_appeared / float(len(text.split()))\n",
        "        idf = np.log((number_of_documents) / float(number_of_times_word_appeared))\n",
        "        tf_idf = tf * idf\n",
        "        return number_of_times_word_appeared, tf, idf, tf_idf\n",
        "\n",
        "    # Calculate tf-idf and other metrics for each keyword\n",
        "    df['number_of_times_word_appeared'] = df['keywords'].apply(lambda x: weightage(x, text)[0])\n",
        "    df['tf'] = df['keywords'].apply(lambda x: weightage(x, text)[1])\n",
        "    df['idf'] = df['keywords'].apply(lambda x: weightage(x, text)[2])\n",
        "    df['tf_idf'] = df['keywords'].apply(lambda x: weightage(x, text)[3])\n",
        "\n",
        "    # Sort the DataFrame by tf-idf\n",
        "    df = df.sort_values('tf_idf', ascending=True)\n",
        "\n",
        "    # Output CSV file name\n",
        "    output_filename = os.path.splitext(os.path.basename(filename))[0] + '_output.csv'\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    df.to_csv(os.path.join(folder_path, output_filename), index=False)\n",
        "    print(f\"Processed {filename} and saved to {output_filename}\")\n",
        "\n",
        "# Iterate over all PDF files in the folder\n",
        "for file in os.listdir(folder_path):\n",
        "    if file.endswith('.pdf'):\n",
        "        process_pdf(os.path.join(folder_path, file))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "394fc0f2",
      "metadata": {
        "id": "394fc0f2",
        "outputId": "485d38d9-22ec-4c97-aafd-fe365e257f47"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Miao.Li\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         keywords  total_frequency\n",
            "0            skin             3540\n",
            "1               0             3243\n",
            "5               1             2559\n",
            "6               2             2378\n",
            "11              h             1870\n",
            "8               3             1638\n",
            "14              5             1358\n",
            "16              4             1347\n",
            "17              j             1337\n",
            "3              al             1287\n",
            "4              et             1256\n",
            "23             10             1141\n",
            "35              6              978\n",
            "2           human              940\n",
            "1206         drug              884\n",
            "12     permeation              880\n",
            "20              c              840\n",
            "159             8              730\n",
            "64    penetration              729\n",
            "9               e              706\n",
            "143             n              676\n",
            "163             p              656\n",
            "42          using              654\n",
            "104             7              639\n",
            "295            ml              635\n"
          ]
        }
      ],
      "source": [
        "#comparison of the csv files\n",
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords if not already present\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Directory containing the output CSV files\n",
        "folder_path = 'C:/Users/Miao.Li/python_learning/pdf scraping'\n",
        "\n",
        "# Dictionary to hold the cumulative data\n",
        "keyword_data = {}\n",
        "\n",
        "# Function to merge keyword data from a CSV file into the cumulative dictionary\n",
        "def merge_keywords(csv_file):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    for _, row in df.iterrows():\n",
        "        keyword = row['keywords']\n",
        "        frequency = row['number_of_times_word_appeared']\n",
        "        # Remove stopwords\n",
        "        if keyword not in stop_words:\n",
        "            if keyword in keyword_data:\n",
        "                keyword_data[keyword] += frequency\n",
        "            else:\n",
        "                keyword_data[keyword] = frequency\n",
        "\n",
        "# Iterate over all output CSV files and merge their data\n",
        "for file in os.listdir(folder_path):\n",
        "    if file.endswith('_output.csv'):\n",
        "        merge_keywords(os.path.join(folder_path, file))\n",
        "\n",
        "# Convert the cumulative dictionary to a DataFrame\n",
        "cumulative_df = pd.DataFrame(list(keyword_data.items()), columns=['keywords', 'total_frequency'])\n",
        "\n",
        "# Sort the DataFrame by total frequency in descending order\n",
        "cumulative_df = cumulative_df.sort_values('total_frequency', ascending=False)\n",
        "\n",
        "# Display the top 25 keywords with the highest total frequency\n",
        "print(cumulative_df.head(25))\n",
        "\n",
        "# Save the cumulative DataFrame to a CSV file\n",
        "cumulative_df.to_csv(os.path.join(folder_path, 'cumulative_keyword_frequency.csv'), index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16a16991",
      "metadata": {
        "id": "16a16991",
        "outputId": "3c4be7d6-04e5-417e-a497-de3ae0996a7c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Miao.Li\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           keywords  total_frequency\n",
            "0              skin             3540\n",
            "1             human              940\n",
            "982            drug              884\n",
            "4        permeation              880\n",
            "37      penetration              729\n",
            "23            using              654\n",
            "14             dose              559\n",
            "25             used              549\n",
            "7          exposure              543\n",
            "9             vitro              514\n",
            "21             time              505\n",
            "10       absorption              502\n",
            "40            water              496\n",
            "24             data              454\n",
            "27            study              428\n",
            "26            table              427\n",
            "1526    transdermal              426\n",
            "46    concentration              418\n",
            "776            acid              403\n",
            "97          applied              401\n",
            "1394       delivery              397\n",
            "64          studies              394\n",
            "3            dermal              390\n",
            "1447            gel              384\n",
            "5          receptor              358\n"
          ]
        }
      ],
      "source": [
        "# remove numbers, words with number of letters less than 3\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords if not already present\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Directory containing the output CSV files\n",
        "folder_path = 'C:/Users/Miao.Li/python_learning/pdf scraping'\n",
        "\n",
        "# Dictionary to hold the cumulative data\n",
        "keyword_data = {}\n",
        "\n",
        "# Function to determine if a keyword should be filtered out\n",
        "def should_exclude(keyword):\n",
        "    # Ensure the keyword is a string\n",
        "    if not isinstance(keyword, str):\n",
        "        return True\n",
        "    # Check if the keyword is a stopword, a single letter, a number, or has fewer than 3 letters\n",
        "    if keyword in stop_words or len(keyword) < 3 or keyword.isdigit():\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Function to merge keyword data from a CSV file into the cumulative dictionary\n",
        "def merge_keywords(csv_file):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    for _, row in df.iterrows():\n",
        "        keyword = row['keywords']\n",
        "        frequency = row['number_of_times_word_appeared']\n",
        "        # Exclude unwanted keywords\n",
        "        if not should_exclude(keyword):\n",
        "            if keyword in keyword_data:\n",
        "                keyword_data[keyword] += frequency\n",
        "            else:\n",
        "                keyword_data[keyword] = frequency\n",
        "\n",
        "# Iterate over all output CSV files and merge their data\n",
        "for file in os.listdir(folder_path):\n",
        "    if file.endswith('_output.csv'):\n",
        "        merge_keywords(os.path.join(folder_path, file))\n",
        "\n",
        "# Convert the cumulative dictionary to a DataFrame\n",
        "cumulative_df = pd.DataFrame(list(keyword_data.items()), columns=['keywords', 'total_frequency'])\n",
        "\n",
        "# Sort the DataFrame by total frequency in descending order\n",
        "cumulative_df = cumulative_df.sort_values('total_frequency', ascending=False)\n",
        "\n",
        "# Display the top 25 keywords with the highest total frequency\n",
        "print(cumulative_df.head(25))\n",
        "\n",
        "# Save the cumulative DataFrame to a CSV file\n",
        "cumulative_df.to_csv(os.path.join(folder_path, 'cumulative_keyword_frequency.csv'), index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "763590a6",
      "metadata": {
        "id": "763590a6",
        "outputId": "fe29fba6-8b3b-4cd8-b0a6-9e2f8311936a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Miao.Li\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           keywords  total_frequency  file_count\n",
            "0              skin             3540          36\n",
            "1             human              940          36\n",
            "982            drug              884          33\n",
            "4        permeation              880          34\n",
            "37      penetration              729          35\n",
            "23            using              654          35\n",
            "14             dose              559          27\n",
            "25             used              549          35\n",
            "7          exposure              543          19\n",
            "9             vitro              514          36\n",
            "21             time              505          35\n",
            "10       absorption              502          33\n",
            "40            water              496          35\n",
            "24             data              454          35\n",
            "27            study              428          35\n",
            "26            table              427          36\n",
            "1526    transdermal              426          30\n",
            "46    concentration              418          34\n",
            "776            acid              403          28\n",
            "97          applied              401          34\n",
            "1394       delivery              397          28\n",
            "64          studies              394          36\n",
            "3            dermal              390          33\n",
            "1447            gel              384          11\n",
            "5          receptor              358          30\n"
          ]
        }
      ],
      "source": [
        "# remove numbers, words with number of letters less than 3\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords if not already present\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Directory containing the output CSV files\n",
        "folder_path = 'C:/Users/Miao.Li/python_learning/pdf scraping'\n",
        "\n",
        "# Dictionary to hold cumulative data: keyword -> [total_frequency, file_count]\n",
        "keyword_data = {}\n",
        "\n",
        "# Function to determine if a keyword should be filtered out\n",
        "def should_exclude(keyword):\n",
        "    # Ensure the keyword is a string\n",
        "    if not isinstance(keyword, str):\n",
        "        return True\n",
        "    # Check if the keyword is a stopword, a single letter, a number, or has fewer than 3 letters\n",
        "    if keyword in stop_words or len(keyword) < 3 or keyword.isdigit():\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Function to merge keyword data from a CSV file into the cumulative dictionary\n",
        "def merge_keywords(csv_file):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    file_keywords = set()  # Track keywords that appear in this file\n",
        "    for _, row in df.iterrows():\n",
        "        keyword = row['keywords']\n",
        "        frequency = row['number_of_times_word_appeared']\n",
        "        # Exclude unwanted keywords\n",
        "        if not should_exclude(keyword):\n",
        "            if keyword in keyword_data:\n",
        "                keyword_data[keyword][0] += frequency\n",
        "                if keyword not in file_keywords:\n",
        "                    keyword_data[keyword][1] += 1\n",
        "            else:\n",
        "                keyword_data[keyword] = [frequency, 1]\n",
        "            file_keywords.add(keyword)\n",
        "\n",
        "# Iterate over all output CSV files and merge their data\n",
        "for file in os.listdir(folder_path):\n",
        "    if file.endswith('_output.csv'):\n",
        "        merge_keywords(os.path.join(folder_path, file))\n",
        "\n",
        "# Convert the cumulative dictionary to a DataFrame\n",
        "cumulative_df = pd.DataFrame(list(keyword_data.items()), columns=['keywords', 'total_frequency_file_count'])\n",
        "cumulative_df[['total_frequency', 'file_count']] = pd.DataFrame(cumulative_df['total_frequency_file_count'].tolist(), index=cumulative_df.index)\n",
        "cumulative_df.drop(columns=['total_frequency_file_count'], inplace=True)\n",
        "\n",
        "# Sort the DataFrame by total frequency in descending order\n",
        "cumulative_df = cumulative_df.sort_values('total_frequency', ascending=False)\n",
        "\n",
        "# Display the top 25 keywords with the highest total frequency\n",
        "print(cumulative_df.head(25))\n",
        "\n",
        "# Save the cumulative DataFrame to a CSV file\n",
        "cumulative_df.to_csv(os.path.join(folder_path, 'cumulative_keyword_frequency.csv'), index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9de4feec",
      "metadata": {
        "id": "9de4feec"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}